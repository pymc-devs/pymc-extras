"""
T7) Model Equivalence

Given a saved LBFGS history (x_full, g_full) and a fixed RNG seed, the ELBO
computation must reproduce the reference values recorded from the pre-refactor
implementation.

Fixture files live in tests/pathfinder/fixtures/ and are generated by running:

    python tests/pathfinder/generate_fixtures.py

Each .npz file contains:
    x_full   – position history  (L+1, N)
    g_full   – gradient history  (L+1, N)
    elbo_ref – reference ELBO    (L,)

Test strategy
-------------
Primary check (same code, same seed):
    Recompute ELBO on the saved history with ELBO_SEED; results must be
    bit-for-bit identical to elbo_ref.

Post-refactor fallback (seeds diverge due to changed RNG streams):
    If the values differ, we verify statistical equivalence:
    - Spearman rank correlation > 0.5
    - argmax within max(5, L//10) steps of the reference argmax
"""

import os

import numpy as np
import pymc as pm
import pytest

from pymc.pytensorf import find_rng_nodes, reseed_rngs
from pytensor.compile.mode import Mode

from pymc_extras.inference.pathfinder.pathfinder import (
    DEFAULT_LINKER,
    get_logp_dlogp_of_ravel_inputs,
    make_elbo_fn,
)
from tests.pathfinder.equivalence_models import MODEL_FACTORIES

# Must stay in sync with generate_fixtures.py
FIXTURES_DIR = os.path.join(os.path.dirname(__file__), "fixtures")
ELBO_SEED = 12345
MAXCOR = 6
NUM_ELBO_DRAWS = 100


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------


def _load_fixture(name: str):
    path = os.path.join(FIXTURES_DIR, f"{name}.npz")
    if not os.path.exists(path):
        pytest.skip(
            f"Fixture not found: {path}. " "Run tests/pathfinder/generate_fixtures.py to create it."
        )
    data = np.load(path)
    return data["x_full"], data["g_full"], data["elbo_ref"]


def _compute_elbo(model: pm.Model, x_full, g_full, seed: int = ELBO_SEED) -> np.ndarray:
    compile_kwargs = {"mode": Mode(linker=DEFAULT_LINKER)}
    logp_dlogp_func = get_logp_dlogp_of_ravel_inputs(model, jacobian=True, **compile_kwargs)

    def logp_func(x):
        logp, _ = logp_dlogp_func(x)
        return logp

    elbo_fn = make_elbo_fn(logp_func, MAXCOR, NUM_ELBO_DRAWS, **compile_kwargs)
    rngs = find_rng_nodes(elbo_fn.maker.fgraph.outputs)
    reseed_rngs(rngs, [seed])
    (elbo,) = elbo_fn(x_full, g_full)
    return elbo


# ---------------------------------------------------------------------------
# Tests
# ---------------------------------------------------------------------------


@pytest.mark.parametrize("model_name", list(MODEL_FACTORIES.keys()))
def test_elbo_all_finite(model_name):
    """Reference ELBO values recorded from current code must all be finite."""
    _, _, elbo_ref = _load_fixture(model_name)
    assert np.all(np.isfinite(elbo_ref)), (
        f"[{model_name}] reference ELBO contains non-finite values: "
        f"{elbo_ref[~np.isfinite(elbo_ref)]}"
    )


@pytest.mark.parametrize("model_name", list(MODEL_FACTORIES.keys()))
def test_elbo_reproducible(model_name):
    """Recomputing ELBO on the saved history with the same seed must match reference exactly."""
    x_full, g_full, elbo_ref = _load_fixture(model_name)
    model = MODEL_FACTORIES[model_name]()
    elbo = _compute_elbo(model, x_full, g_full)

    assert (
        elbo.shape == elbo_ref.shape
    ), f"[{model_name}] shape mismatch: {elbo.shape} vs {elbo_ref.shape}"
    assert np.all(np.isfinite(elbo)), f"[{model_name}] recomputed ELBO contains non-finite values"
    np.testing.assert_array_equal(
        elbo,
        elbo_ref,
        err_msg=f"[{model_name}] ELBO values differ from reference (seed-aligned run should be identical)",
    )


def _check_statistical_equivalence(model_name, elbo, elbo_ref):
    """Shared helper: assert that elbo is statistically consistent with elbo_ref."""
    from scipy.stats import spearmanr

    L = len(elbo_ref)
    if L < 2:
        pytest.skip(f"[{model_name}] only {L} ELBO step(s), statistical check not meaningful")

    assert np.all(np.isfinite(elbo)), f"[{model_name}] ELBO contains non-finite values"

    corr, _ = spearmanr(elbo, elbo_ref)
    assert corr > 0.5, (
        f"[{model_name}] Spearman rank correlation {corr:.3f} < 0.5; "
        "ELBO ordering is inconsistent with reference"
    )

    argmax_new = int(np.argmax(elbo))
    argmax_ref = int(np.nanargmax(elbo_ref))
    tol = max(5, L // 10)
    assert abs(argmax_new - argmax_ref) <= tol, (
        f"[{model_name}] argmax {argmax_new} is more than {tol} steps from "
        f"reference argmax {argmax_ref} (L={L})"
    )


@pytest.mark.parametrize("model_name", list(MODEL_FACTORIES.keys()))
def test_elbo_statistical_equivalence(model_name):
    """
    Post-refactor guard: if the exact values diverge (e.g. due to changed RNG
    streams), statistical equivalence is required.

    This test is intentionally lenient so it keeps passing after a refactor
    that changes the order of RNG draws while preserving algorithm correctness.
    """
    x_full, g_full, elbo_ref = _load_fixture(model_name)
    model = MODEL_FACTORIES[model_name]()
    elbo = _compute_elbo(model, x_full, g_full)
    _check_statistical_equivalence(model_name, elbo, elbo_ref)


@pytest.mark.parametrize("model_name", list(MODEL_FACTORIES.keys()))
def test_elbo_statistical_equivalence_different_seed(model_name):
    """
    Verify the statistical-equivalence tolerances are meaningful by using a
    seed that is deliberately different from the one used to generate the
    fixtures.  The draws change, but the underlying local approximations are
    the same, so rank ordering and argmax should still match within tolerance.

    This also validates that the statistical-equivalence checks are not
    trivially satisfied only when seeds match.
    """
    x_full, g_full, elbo_ref = _load_fixture(model_name)
    model = MODEL_FACTORIES[model_name]()
    elbo = _compute_elbo(model, x_full, g_full, seed=ELBO_SEED + 9999)
    _check_statistical_equivalence(model_name, elbo, elbo_ref)
