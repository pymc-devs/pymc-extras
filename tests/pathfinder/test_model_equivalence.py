"""
Model Equivalence

Full-stack test: LBFGSStreamingCallback is invoked with the fixture trajectory
(just like scipy.minimize would), computing alpha, s, z internally. ELBO from
the callback must be statistically equivalent to the reference.

Fixture files live in tests/pathfinder/fixtures/ and are generated by running:

    python tests/pathfinder/generate_fixtures.py

Each .npz file contains:
    x_full     – position history  (L+1, N)
    g_full     – gradient history  (L+1, N)
    elbo_ref   – reference ELBO    (L,)

Test strategy
-------------
Statistical equivalence (platform-independent):
    - Spearman rank correlation > 0.75
    - argmax within max(5, L//5) steps of the reference argmax
    - Ballpark: 90% of ELBO values within 20% (relative) of reference
"""

import os

from unittest.mock import MagicMock, patch

import numpy as np
import pymc as pm
import pytest

from pymc.blocking import DictToArrayBijection
from pymc.initial_point import make_initial_point_fn
from pymc.model.core import Point
from pytensor.compile.mode import Mode

from pymc_extras.inference.pathfinder.lbfgs import LBFGSStatus, _CachedValueGrad
from pymc_extras.inference.pathfinder.pathfinder import (
    DEFAULT_LINKER,
    FAILED_PATH_STATUS,
    LBFGSStreamingCallback,
    get_logp_dlogp_of_ravel_inputs,
    make_pathfinder_sample_fn,
    make_single_pathfinder_fn,
)
from tests.pathfinder.equivalence_models import MODEL_FACTORIES, make_ard_regression

# Must stay in sync with generate_fixtures.py
FIXTURES_DIR = os.path.join(os.path.dirname(__file__), "fixtures")
ELBO_SEED = 12345
MAXCOR = 6
MAXITER = 100
NUM_DRAWS = 50
NUM_ELBO_DRAWS = 100
JITTER = 2.0
EPSILON = 1e-12

COMPILE_KWARGS = {"mode": Mode(linker=DEFAULT_LINKER)}


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------


def _load_fixture(name: str):
    path = os.path.join(FIXTURES_DIR, f"{name}.npz")
    if not os.path.exists(path):
        pytest.skip(
            f"Fixture not found: {path}. Run tests/pathfinder/generate_fixtures.py to create it."
        )
    with np.load(path) as data:
        required = ("x_full", "g_full", "elbo_ref")
        if not all(k in data for k in required):
            pytest.skip(
                f"Fixture {path} is outdated. "
                "Re-run tests/pathfinder/generate_fixtures.py to regenerate."
            )
        return data["x_full"], data["g_full"], data["elbo_ref"]


def _run_callback_replay(
    model: pm.Model,
    x_full: np.ndarray,
    seed: int = ELBO_SEED,
) -> np.ndarray:
    """Replay x_full through LBFGSStreamingCallback (full stack: alpha, s, z computed internally)."""
    logp_dlogp = get_logp_dlogp_of_ravel_inputs(model, jacobian=True, **COMPILE_KWARGS)

    def neg_logp_dlogp_func(x):
        logp, dlogp = logp_dlogp(x)
        return -logp, -dlogp

    N = x_full.shape[1]
    sample_logp_fn = make_pathfinder_sample_fn(
        model, N, MAXCOR, jacobian=True, compile_kwargs=COMPILE_KWARGS
    )
    cached_fn = _CachedValueGrad(neg_logp_dlogp_func)
    rng = np.random.default_rng(seed)

    elbo_history = []

    def on_step(x, g, alpha, s_win, z_win, elbo):
        elbo_history.append(elbo if np.isfinite(elbo) else np.nan)

    cb = LBFGSStreamingCallback(
        value_grad_fn=cached_fn,
        x0=x_full[0],
        sample_logp_fn=sample_logp_fn,
        num_elbo_draws=NUM_ELBO_DRAWS,
        rng=rng,
        J=MAXCOR,
        epsilon=EPSILON,
        on_step_callback=on_step,
    )

    # Replay trajectory (same as scipy.minimize callback invocation)
    _, g_init = cached_fn(x_full[0])
    cb.x_prev = x_full[0].copy()
    cb.g_prev = np.array(g_init, dtype=np.float64)

    for x_k in x_full[1:]:
        cached_fn(x_k)  # warm cache
        cb(x_k)

    return np.array(elbo_history)


def _make_single_fn(model, maxiter: int = MAXITER, maxcor: int = MAXCOR):
    return make_single_pathfinder_fn(
        model=model,
        num_draws=NUM_DRAWS,
        maxcor=maxcor,
        maxiter=maxiter,
        ftol=1e-5,
        gtol=1e-8,
        maxls=1000,
        num_elbo_draws=NUM_ELBO_DRAWS,
        jitter=JITTER,
        epsilon=1e-8,
        compile_kwargs=COMPILE_KWARGS,
    )


def _run_path(fn, seed: int = 42):
    return fn(seed)


def _build_logp_and_neg(model, compile_kwargs=COMPILE_KWARGS):
    logp_dlogp = get_logp_dlogp_of_ravel_inputs(model, jacobian=True, **compile_kwargs)

    def neg_logp_dlogp_func(x):
        v, dv = logp_dlogp(x)
        return -v, -dv

    return None, neg_logp_dlogp_func


def _make_lbfgs_mock(x_full):
    """Patch LBFGS to replay x_full[1:] through the streaming callback."""
    mock_inst = MagicMock()

    def replay_streaming(callback, x0):
        _, g_init = callback.value_grad_fn(x_full[0])
        callback.x_prev = x_full[0].copy()
        callback.g_prev = np.array(g_init, dtype=np.float64)

        for x_k in x_full[1:]:
            callback.value_grad_fn(x_k)
            callback(x_k)

        return x_full.shape[0] - 1, LBFGSStatus.CONVERGED

    mock_inst.minimize_streaming.side_effect = replay_streaming

    return patch("pymc_extras.inference.pathfinder.pathfinder.LBFGS", return_value=mock_inst)


# ---------------------------------------------------------------------------
# Tests
# ---------------------------------------------------------------------------


def test_elbo_call_count():
    """sample_logp_fn is called exactly once per accepted LBFGS step."""
    model = make_ard_regression()
    _, neg_logp_dlogp_func = _build_logp_and_neg(model)
    N = DictToArrayBijection.map(model.initial_point()).data.shape[0]

    sample_logp_fn = make_pathfinder_sample_fn(
        model, N, MAXCOR, jacobian=True, compile_kwargs=COMPILE_KWARGS
    )

    call_count = [0]

    def counting_fn(x, g, alpha, s_win, z_win, u):
        call_count[0] += 1
        return sample_logp_fn(x, g, alpha, s_win, z_win, u)

    ipfn = make_initial_point_fn(model=model)
    ip = Point(ipfn(None), model=model)
    x_base = DictToArrayBijection.map(ip).data
    rng = np.random.default_rng(0)
    x0 = x_base + rng.uniform(-JITTER, JITTER, size=x_base.shape)

    cached_fn = _CachedValueGrad(neg_logp_dlogp_func)
    cb = LBFGSStreamingCallback(
        value_grad_fn=cached_fn,
        x0=x0,
        sample_logp_fn=counting_fn,
        num_elbo_draws=NUM_ELBO_DRAWS,
        rng=rng,
        J=MAXCOR,
        epsilon=1e-8,
    )

    from scipy.optimize import minimize as scipy_minimize

    scipy_minimize(
        cached_fn,
        x0,
        method="L-BFGS-B",
        jac=True,
        callback=cb,
        options={"maxcor": MAXCOR, "maxiter": MAXITER, "ftol": 1e-5, "gtol": 1e-8, "maxls": 1000},
    )

    assert call_count[0] == cb.step_count
    assert cb.step_count > 0


@pytest.mark.parametrize("model_name", ["ard_regression", "hd_gaussian"])
def test_rng_reproducibility(model_name):
    """Two streaming runs with the same seed produce bit-identical results."""
    model = MODEL_FACTORIES[model_name]()
    fn = _make_single_fn(model)

    r1 = _run_path(fn, seed=123)
    r2 = _run_path(fn, seed=123)

    assert r1.path_status not in FAILED_PATH_STATUS
    assert r2.path_status not in FAILED_PATH_STATUS

    np.testing.assert_array_equal(r1.samples, r2.samples)
    np.testing.assert_array_equal(r1.logP, r2.logP)
    np.testing.assert_array_equal(r1.logQ, r2.logQ)
    assert r1.best_step_idx == r2.best_step_idx if hasattr(r1, "best_step_idx") else True


@pytest.mark.parametrize("model_name", list(MODEL_FACTORIES.keys()))
def test_fixture_match(model_name):
    """Streaming path selects an ELBO argmax consistent with the fixture.

    LBFGS is mocked so the streaming path processes the exact same trajectory
    stored in the fixture. ELBO argmax must fall within a generous window.
    """
    x_full, _, elbo_ref = _load_fixture(model_name)
    ref_argmax = int(np.nanargmax(elbo_ref))
    L = x_full.shape[0] - 1

    model = MODEL_FACTORIES[model_name]()
    N = DictToArrayBijection.map(model.initial_point()).data.shape[0]

    with _make_lbfgs_mock(x_full):
        fn = _make_single_fn(model)
        result = _run_path(fn, seed=42)

    tag = f"[{model_name}]"

    assert result.path_status not in FAILED_PATH_STATUS, f"{tag} path failed: {result.path_status}"
    assert result.samples is not None
    assert result.samples.shape == (1, NUM_DRAWS, N)
    assert np.any(np.isfinite(result.logP)), f"{tag} all logP are non-finite"
    assert np.any(np.isfinite(result.logQ)), f"{tag} all logQ are non-finite"

    argmax = int(result.elbo_argmax)
    tolerance = max(3, L // 4)
    assert abs(argmax - ref_argmax) <= tolerance, (
        f"{tag} elbo_argmax={argmax} too far from fixture ref_argmax={ref_argmax} "
        f"(L={L}, tolerance={tolerance})"
    )


@pytest.mark.parametrize("model_name", list(MODEL_FACTORIES.keys()))
def test_elbo_all_finite(model_name):
    """Reference ELBO values recorded from current code must all be finite."""
    *_, elbo_ref = _load_fixture(model_name)
    assert np.all(np.isfinite(elbo_ref)), (
        f"[{model_name}] reference ELBO contains non-finite values: "
        f"{elbo_ref[~np.isfinite(elbo_ref)]}"
    )


def _check_statistical_equivalence(
    model_name,
    elbo,
    elbo_ref,
    min_corr: float = 0.85,
    min_ballpark_fraction: float = 0.8,
    max_rel_diff: float = 0.1,
):
    """Shared helper: assert that elbo is statistically consistent with elbo_ref.

    All checks must pass; no platform-specific randomness guarantees.
    """
    from scipy.stats import spearmanr

    L = len(elbo_ref)
    if L < 2:
        pytest.skip(f"[{model_name}] only {L} ELBO step(s), statistical check not meaningful")

    assert np.all(np.isfinite(elbo)), f"[{model_name}] ELBO contains non-finite values"

    corr, _ = spearmanr(elbo, elbo_ref)
    assert corr > min_corr, (
        f"[{model_name}] Spearman rank correlation {corr:.3f} < {min_corr}; "
        "ELBO ordering is inconsistent with reference"
    )

    argmax_new = int(np.argmax(elbo))
    argmax_ref = int(np.nanargmax(elbo_ref))
    tol = max(5, L // 5)
    assert abs(argmax_new - argmax_ref) <= tol, (
        f"[{model_name}] argmax {argmax_new} is more than {tol} steps from "
        f"reference argmax {argmax_ref} (L={L})"
    )

    # Ballpark: fraction of values within max_rel_diff of reference
    denom = np.abs(elbo_ref) + 1e-10
    rel_diff = np.abs(elbo - elbo_ref) / denom
    within = np.isfinite(rel_diff) & (rel_diff <= max_rel_diff)
    fraction = np.mean(within)
    pairs = "\n".join(
        f"  step {i}: elbo={e:.6g}, elbo_ref={r:.6g}, rel_diff={d:.4f}"
        for i, (e, r, d) in enumerate(zip(elbo, elbo_ref, rel_diff))
    )
    assert fraction >= min_ballpark_fraction, (
        f"[{model_name}] only {fraction:.1%} of ELBO values within {max_rel_diff:.0%} of "
        f"reference (required {min_ballpark_fraction:.0%})\n"
        f"Full trace (elbo, elbo_ref) pairs:\n{pairs}"
    )


@pytest.mark.parametrize("model_name", list(MODEL_FACTORIES.keys()))
def test_elbo_statistical_equivalence(model_name):
    """Full-stack: LBFGSStreamingCallback computes alpha/s/z internally; ELBO matches reference."""
    x_full, g_full, elbo_ref = _load_fixture(model_name)
    model = MODEL_FACTORIES[model_name]()
    elbo = _run_callback_replay(model, x_full)
    assert (
        elbo.shape == elbo_ref.shape
    ), f"[{model_name}] shape mismatch: {elbo.shape} vs {elbo_ref.shape}"
    _check_statistical_equivalence(model_name, elbo, elbo_ref)


@pytest.mark.parametrize("model_name", list(MODEL_FACTORIES.keys()))
def test_elbo_statistical_equivalence_different_seed(model_name):
    """Verify tolerances are meaningful by using a different ELBO seed."""
    x_full, _, elbo_ref = _load_fixture(model_name)
    model = MODEL_FACTORIES[model_name]()
    elbo = _run_callback_replay(model, x_full, seed=ELBO_SEED + 9999)
    assert elbo.shape == elbo_ref.shape
    _check_statistical_equivalence(model_name, elbo, elbo_ref)
